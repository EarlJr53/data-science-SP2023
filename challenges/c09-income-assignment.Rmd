---
title: "US Income"
author: "Brooke Moss"
date: 2023-04-09
output:
  github_document:
    toc: true
prerequisites:
  - e-stat09-bootstrap
---

*Purpose*: We've been learning how to quantify uncertainty in estimates
through the exercises; now its time to put those skills to use studying
real data. In this challenge we'll use concepts like confidence
intervals to help us make sense of census data.

*Reading*: - [Using ACS Estimates and Margin of
Error](https://www.census.gov/data/academy/webinars/2020/calculating-margins-of-error-acs.html)
(Optional, see the PDF on the page) - [Patterns and Causes of
Uncertainty in the American Community
Survey](https://www.sciencedirect.com/science/article/pii/S0143622813002518?casa_token=VddzQ1-spHMAAAAA:FTq92LXgiPVloJUVjnHs8Ma1HwvPigisAYtzfqaGbbRRwoknNq56Y2IzszmGgIGH4JAPzQN0)
(Optional, particularly the *Uncertainty in surveys* section under the
Introduction.)

<!-- include-rubric -->

# Grading Rubric

<!-- -------------------------------------------------- -->

Unlike exercises, **challenges will be graded**. The following rubrics
define how you will be graded, both on an individual and team basis.

## Individual

<!-- ------------------------- -->

| Category    | Needs Improvement                                                                                                | Satisfactory                                                                                                               |
|------------------|-----------------------------|-------------------------|
| Effort      | Some task **q**'s left unattempted                                                                               | All task **q**'s attempted                                                                                                 |
| Observed    | Did not document observations, or observations incorrect                                                         | Documented correct observations based on analysis                                                                          |
| Supported   | Some observations not clearly supported by analysis                                                              | All observations clearly supported by analysis (table, graph, etc.)                                                        |
| Assessed    | Observations include claims not supported by the data, or reflect a level of certainty not warranted by the data | Observations are appropriately qualified by the quality & relevance of the data and (in)conclusiveness of the support      |
| Specified   | Uses the phrase "more data are necessary" without clarification                                                  | Any statement that "more data are necessary" specifies which *specific* data are needed to answer what *specific* question |
| Code Styled | Violations of the [style guide](https://style.tidyverse.org/) hinder readability                                 | Code sufficiently close to the [style guide](https://style.tidyverse.org/)                                                 |

## Submission

<!-- ------------------------- -->

Make sure to commit both the challenge report (`report.md` file) and
supporting files (`report_files/` folder) when you are done! Then submit
a link to Canvas. **Your Challenge submission is not complete without
all files uploaded to GitHub.**

# Setup

<!-- ----------------------------------------------------------------------- -->

```{r setup}
library(tidyverse)
```

### **q1** Load the population data from c06; simply replace `filename_pop` below.

```{r q1-task}
## TODO: Give the filename for your copy of Table B01003
filename_pop <- "./data/ACSDT5Y2018.B01003-Data.csv"

## NOTE: No need to edit
df_pop <-
  read_csv(
    filename_pop,
    skip = 1,
  ) %>% 
  rename(
    population_estimate = `Estimate!!Total`,
    geographic_area_name = `Geographic Area Name`
  )

df_pop
```

You might wonder why the `Margin of Error` in the population estimates
is listed as `*****`. From the [documentation (PDF
link)](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwj81Omy16TrAhXsguAKHTzKDQEQFjABegQIBxAB&url=https%3A%2F%2Fwww2.census.gov%2Fprograms-surveys%2Facs%2Ftech_docs%2Faccuracy%2FMultiyearACSAccuracyofData2018.pdf%3F&usg=AOvVaw2TOrVuBDlkDI2gde6ugce_)
for the ACS:

> If the margin of error is displayed as '\*\*\*\*\*' (five asterisks),
> the estimate has been controlled to be equal to a fixed value and so
> it has no sampling error. A standard error of zero should be used for
> these controlled estimates when completing calculations, such as those
> in the following section.

This means that for cases listed as `*****` the US Census Bureau
recommends treating the margin of error (and thus standard error) as
zero.

### **q2** Obtain median income data from the Census Bureau:

-   `Filter > Topics > Income and Poverty > Income and Poverty`
-   `Filter > Geography > County > All counties in United States`
-   Look for `Median Income in the Past 12 Months` (Table S1903)
-   Download the 2018 5-year ACS estimates; save to your `data` folder
    and add the filename below.

```{r q2-task}
## TODO: Give the filename for your copy of Table S1903
filename_income <- "./data/ACSST5Y2018.S1903-Data.csv"

## NOTE: No need to edit
df_income <-
  read_csv(filename_income, skip = 1)
df_income
```

Use the following test to check that you downloaded the correct file:

```{r q2-tests}
## NOTE: No need to edit, use to check you got the right file.
assertthat::assert_that(
  df_income %>%
    filter(Geography == "0500000US01001") %>%
    pull(`Estimate!!Percent Distribution!!FAMILY INCOME BY FAMILY SIZE!!2-person families`)
  == 45.6
)

print("Well done!")
```

This dataset is in desperate need of some *tidying*. To simplify the
task, we'll start by considering the `\\d-person families` columns
first.

### **q3** Tidy the `df_income` dataset by completing the code below. Pivot and rename the columns to arrive at the column names `id, geographic_area_name, category, income_estimate, income_moe`.

*Hint*: You can do this in a single pivot using the `".value"` argument
and a `names_pattern` using capture groups `"()"`. Remember that you can
use an OR operator `|` in a regex to allow for multiple possibilities in
a capture group, for example `"(Estimate|Margin of Error)"`.

```{r q3-task}
df_q3 <-
  df_income %>%
  select(
    Geography,
    contains("Geographic"),
    # This will select only the numeric d-person family columns;
    # it will ignore the annotation columns
    contains("median") & matches("\\d-person families") & !contains("Annotation of")
  ) %>%
  mutate(across(contains("median"), as.numeric)) %>%
## TODO: Pivot the data, rename the columns
  pivot_longer(
    cols = contains("median"),
    names_pattern = "(Margin of Error|Estimate)!!(?:.+)!!(.+)",
    names_to = c(".value", "category")
  ) %>%
  rename(geographic_area_name = "Geographic Area Name", income_estimate = "Estimate", income_moe = "Margin of Error") %>%
  glimpse()
```

Use the following tests to check your work:

```{r q3-tests}
## NOTE: No need to edit
assertthat::assert_that(setequal(
  names(df_q3),
  c("Geography", "geographic_area_name", "category", "income_estimate", "income_moe")
))

assertthat::assert_that(
  df_q3 %>%
    filter(Geography == "0500000US01001", category == "2-person families") %>%
    pull(income_moe)
  == 6663
)

print("Nice!")
```

The data gives finite values for the Margin of Error, which is closely
related to the Standard Error. The Census Bureau documentation gives the
following relationship between Margin of Error and Standard Error:

$$\text{MOE} = 1.645 \times \text{SE}.$$

### **q4** Convert the margin of error to standard error. Additionally, compute a 99% confidence interval on income, and normalize the standard error to `income_CV = income_SE / income_estimate`. Provide these columns with the names `income_SE, income_lo, income_hi, income_CV`.

```{r q4-task}

df_q4 <-
  df_q3 %>%
  mutate(
    income_SE = income_moe / 1.645,
    income_CV = income_SE / income_estimate,
    income_lo = income_estimate - 2.58 * income_SE,
    income_hi = income_estimate + 2.58 * income_SE
  )
```

Use the following tests to check your work:

```{r q4-tests}
## NOTE: No need to edit
assertthat::assert_that(setequal(
  names(df_q4),
  c("Geography", "geographic_area_name", "category", "income_estimate", "income_moe",
    "income_SE", "income_lo", "income_hi", "income_CV")
))

assertthat::assert_that(
  abs(
    df_q4 %>%
    filter(Geography == "0500000US01001", category == "2-person families") %>%
    pull(income_SE) - 4050.456
  ) / 4050.456 < 1e-3
)

assertthat::assert_that(
  abs(
    df_q4 %>%
    filter(Geography == "0500000US01001", category == "2-person families") %>%
    pull(income_lo) - 54513.72
  ) / 54513.72 < 1e-3
)

assertthat::assert_that(
  abs(
    df_q4 %>%
    filter(Geography == "0500000US01001", category == "2-person families") %>%
    pull(income_hi) - 75380.28
  ) / 75380.28 < 1e-3
)

assertthat::assert_that(
  abs(
    df_q4 %>%
    filter(Geography == "0500000US01001", category == "2-person families") %>%
    pull(income_CV) - 0.06236556
  ) / 0.06236556 < 1e-3
)

print("Nice!")
```

One last wrangling step: We need to join the two datasets so we can
compare population with income.

### **q5** Join `df_q4` and `df_pop`.

```{r q5-task}
## TODO: Join df_q4 and df_pop by the appropriate column

df_data <-
  left_join(df_pop, df_q4, by = c("Geography", "geographic_area_name")) %>%
  glimpse()
```

# Analysis

<!-- ----------------------------------------------------------------------- -->

We now have both estimates and confidence intervals for
`\\d-person families`. Now we can compare cases with quantified
uncertainties: Let's practice!

### **q6** Study the following graph, making sure to note what you can *and can't* conclude based on the estimates and confidence intervals. Document your observations below and answer the questions.

```{r q6-task}
## NOTE: No need to edit; run and inspect
wid <- 0.5

df_data %>%
  filter(str_detect(geographic_area_name, "Massachusetts")) %>%
  mutate(
    county = str_remove(geographic_area_name, " County,.*$"),
    county = fct_reorder(county, income_estimate)
  ) %>%

  ggplot(aes(county, income_estimate, color = category)) +
  geom_errorbar(
    aes(ymin = income_lo, ymax = income_hi),
    position = position_dodge(width = wid)
  ) +
  geom_point(position = position_dodge(width = wid)) +

  coord_flip() +
  labs(
    x = "County",
    y = "Median Household Income"
  )
```

**Observations**:

-   Document your observations here.
    -   6-person families seem to have the greatest variance and
        2-person families the least, nearly across the board.
    -   The median income appears to increase with the number of people
        in the household, although not in all cases.
-   Can you confidently distinguish between household incomes in Suffolk
    county? Why or why not?
    -   No, I cannot confidently distinguish between household incomes
        in Suffolk county, because all of the household sizes in Suffolk
        have very similar median incomes and ranges of incomes.
-   Which counties have the widest confidence intervals?
    -   Nantucket and Dukes counties, the two wealthiest in the state,
        have the widest confidence intervals.
    -   Franklin, Berkshire, and Hampshire counties also have relatively
        wide confidence intervals, but only for certain household sizes.

In the next task you'll investigate the relationship between population
and uncertainty.

### **q7** Plot the standard error against population for all counties. Create a visual that effectively highlights the trends in the data. Answer the questions under *observations* below.

*Hint*: Remember that standard error is a function of *both* variability
(e.g. variance) and sample size.

```{r q7-task}
df_data %>%
  group_by(Geography) %>%
  summarize(population = mean(population_estimate), se = mean(income_SE)) %>%
  ggplot(aes(
    x = population,
    y = se
  )) +
  geom_point() +
  geom_smooth() +
  scale_y_log10() +
  scale_x_log10()
```

**Observations**:

-   What *overall* trend do you see between `SE` and population? Why
    might this trend exist?
    -   I see the trend that the larger the population of the county,
        the lower the Standard Error. This is likely because larger
        populations provide more and more data-points, bringing
        estimates and calculations closer to the true value.
-   What does this *overall* trend tell you about the relative ease of
    studying small vs large counties?
    -   This demonstrates that studying small counties is prone to
        additional error that is not present in larger counties.

# Going Further

<!-- ----------------------------------------------------------------------- -->

Now it's your turn! You have income data for every county in the United
States: Pose your own question and try to answer it with the data.

### **q8** Pose your own question about the data. Create a visualization (or table) here, and document your observations.

```{r q8-task}
## TODO: Pose and answer your own question about the data
df_q8 <-
  df_income %>%
  select(
    Geography,
    contains("Geographic"),
    contains("median") & contains("Living alone") & !contains("Annotation of")
  ) %>%
  mutate(across(contains("median"), as.numeric)) %>%
  pivot_longer(
    cols = contains("median"),
    names_pattern = "(Margin of Error|Estimate)!!(?:.+)!!(.+)",
    names_to = c(".value", "category")
  ) %>%
  rename(geographic_area_name = "Geographic Area Name", income_estimate = "Estimate", income_moe = "Margin of Error") %>%
  group_by(geographic_area_name, Geography, category) %>%
  summarize(
    income_estimate = mean(income_estimate),
    income_moe = mean(income_moe)
  ) %>%
  ungroup() %>%
  right_join(df_pop, by = c("Geography", "geographic_area_name"))


df_q8 %>%
  filter(str_detect(geographic_area_name, "Maine")) %>%
  mutate(
    county = str_remove(geographic_area_name, " County,.*$"),
    county = fct_reorder(county, income_estimate)
  ) %>%
  
  ggplot(aes(income_estimate, county, size = population_estimate, color = category)) +
  geom_point() +
  # coord_flip() +
  labs(
    y = "County",
    x = "Median Household Income"
  )
```

**Observations**:

-   As the median income increases, the income disparity between living
    alone and not living alone increases.
-   Androscoggin County, compared to other counties of similar median
    incomes, has a wider disparity.
-   More of the large-population counties have higher median incomes
    than low.

Ideas:

-   Compare trends across counties that are relevant to you; e.g. places
    you've lived, places you've been, places in the US that are
    interesting to you.
-   In q3 we tidied the median `\\d-person families` columns only.
    -   Tidy the other median columns to learn about other people
        groups.
    -   Tidy the percentage columns to learn about how many households
        of each category are in each county.
-   Your own idea!

# References

<!-- ----------------------------------------------------------------------- -->

[1] Spielman SE, Folch DC, Nagle NN (2014) Patterns and causes of
uncertainty in the American Community Survey. Applied Geography 46:
147--157. <pmid:25404783>
[link](https://www.sciencedirect.com/science/article/pii/S0143622813002518?casa_token=VddzQ1-spHMAAAAA:FTq92LXgiPVloJUVjnHs8Ma1HwvPigisAYtzfqaGbbRRwoknNqZ6Y2IzszmGgIGH4JAPzQN0)
